{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90359ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d620cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c9b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "gem_api='AIzaSyBYa9nWB44qIArzY1ZX4vGgAfiQmUa6Urs'\n",
    "genai.configure(api_key=gem_api)\n",
    "#model = genai.GenerativeModel('gemini-exp-1206') # For gemini 2.0 Exp adv\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')# Or use 'gemini-1.5-flash'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09b762b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter(txt):\n",
    "  rsp = model.generate_content(txt)\n",
    "  return rsp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068b6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**a) What is the density-based clustering algorithm? Explain its key concepts.**\n",
      "\n",
      "Density-based clustering algorithms group data points based on the idea that clusters are dense regions separated by sparser regions.  Unlike centroid-based methods like k-means, they don't assume clusters are spherical and can discover clusters of arbitrary shapes.\n",
      "\n",
      "Key Concepts:\n",
      "\n",
      "* **Density:**  The number of data points within a specified radius (Eps) of a given point.\n",
      "* **Core Point:** A data point with at least a specified minimum number of neighbors (MinPts) within its Eps radius (including itself).\n",
      "* **Border Point:** A data point within the Eps radius of a core point but having fewer than MinPts neighbors.\n",
      "* **Noise Point (Outlier):** A data point that is neither a core point nor a border point.\n",
      "* **Directly Density-Reachable:** A point p is directly density-reachable from a point q if p is within the Eps radius of q, and q is a core point.\n",
      "* **Density-Connected:**  Two points p and q are density-connected if there is a chain of points p1, p2, ..., pn, where p1 = p and pn = q, such that pi+1 is directly density-reachable from pi.\n",
      "\n",
      "**Example:** Imagine a scatter plot of stars.  A dense cluster of stars would be identified as a single cluster, even if it's elongated or irregularly shaped.  Individual, isolated stars would be classified as noise.\n",
      "\n",
      "\n",
      "**b) Explain Principal Component Analysis (PCA). How does PCA work, and what are its key applications?**\n",
      "\n",
      "PCA is a dimensionality reduction technique that transforms a dataset into a new coordinate system where the principal components (PCs) represent the directions of maximum variance in the data.  It aims to capture most of the information in the original data using fewer variables (PCs).\n",
      "\n",
      "How PCA works:\n",
      "\n",
      "1. **Standardize the data:** Center the data around zero and scale each variable to have unit variance.\n",
      "2. **Calculate the covariance matrix:**  This matrix shows the relationships between the variables.\n",
      "3. **Compute the eigenvectors and eigenvalues of the covariance matrix:** Eigenvectors represent the directions (PCs) and eigenvalues represent the amount of variance explained by each PC.\n",
      "4. **Select the top k eigenvectors:** Choose the eigenvectors corresponding to the largest eigenvalues. These form the new feature space.\n",
      "5. **Project the data onto the new feature space:** Transform the original data into the lower-dimensional space defined by the selected PCs.\n",
      "\n",
      "Key Applications:\n",
      "\n",
      "* **Dimensionality reduction:** Reduce the number of variables while preserving most of the information.  This can improve the performance of machine learning algorithms and reduce computational cost.\n",
      "* **Noise reduction:** By focusing on the directions of largest variance, PCA can filter out noise.\n",
      "* **Data visualization:** Projecting data onto 2 or 3 PCs allows for visualization of high-dimensional data.\n",
      "* **Feature engineering:**  PCs can be used as new features for machine learning models.\n",
      "\n",
      "**Example:**  In image compression, PCA can represent images using fewer components while retaining most of the visual information.\n",
      "\n",
      "\n",
      "**c) Describe a content-based recommendation system. How does it work, and what are its main advantages and limitations?**\n",
      "\n",
      "A content-based recommendation system suggests items similar to what a user has liked in the past. It focuses on the characteristics of the items themselves rather than user interactions.\n",
      "\n",
      "How it works:\n",
      "\n",
      "1. **Item profiling:** Create a profile for each item, representing its relevant features (e.g., genre for movies, ingredients for recipes, keywords for articles).\n",
      "2. **User profiling:** Build a profile for each user based on their past preferences and interactions (e.g., ratings, purchases, views). This profile reflects the user's preferred item features.\n",
      "3. **Matching:** Recommend items whose profiles are similar to the user's profile. Similarity is calculated using various techniques like cosine similarity.\n",
      "\n",
      "Advantages:\n",
      "\n",
      "* **No cold start problem for new items:**  Recommendations can be made for new items as soon as their profiles are created.\n",
      "* **Personalized recommendations:** Tailored specifically to individual user tastes.\n",
      "* **Transparency:**  Users can understand why an item is recommended.\n",
      "\n",
      "Limitations:\n",
      "\n",
      "* **Over-specialization:**  Recommendations may be too narrow and fail to introduce users to new interests.\n",
      "* **Limited serendipity:** Difficulty in discovering unexpected but relevant items.\n",
      "* **Cold start problem for new users:**  Requires sufficient user interaction data to build an accurate user profile.\n",
      "\n",
      "\n",
      "**Example:**  A music streaming service recommending songs with similar genres, artists, or moods to what a user has previously listened to.\n",
      "\n",
      "\n",
      "\n",
      "**d) What is Market Basket Analysis, how is it used to uncover relationships between items in transactional data? Explain the key concepts and techniques involved.**\n",
      "\n",
      "Market Basket Analysis (MBA) is a technique used to uncover associations between items frequently purchased together in transactional data (e.g., supermarket receipts, online shopping carts). It aims to identify patterns like \"customers who buy X are also likely to buy Y.\"\n",
      "\n",
      "Key Concepts:\n",
      "\n",
      "* **Itemset:** A collection of one or more items.\n",
      "* **Support:** The frequency of an itemset appearing in the dataset.\n",
      "* **Confidence:**  The probability of purchasing item Y given that item X is already in the basket.  (Support(X and Y) / Support(X))\n",
      "* **Lift:**  Measures how much more likely it is to buy Y when X is also purchased compared to buying Y alone. (Support(X and Y) / (Support(X) * Support(Y)))\n",
      "\n",
      "Techniques:\n",
      "\n",
      "* **Apriori Algorithm:**  A widely used algorithm for finding frequent itemsets. It uses the principle that if an itemset is frequent, then all its subsets must also be frequent.\n",
      "* **FP-Growth Algorithm:** A more efficient algorithm than Apriori, especially for large datasets.\n",
      "\n",
      "**Example:** A supermarket analyzes its sales data and discovers that customers who buy diapers are also likely to buy beer.  This information can be used for product placement, targeted promotions, and inventory management.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"Answer the questions where each of them weigh 5 marks, no codes required, only theoritical answers with example\n",
    "a) What is the density-based clustering algorithm? Explain its key concepts. (5 marks)\n",
    "\n",
    "b) Explain Principal Component Analysis (PCA). How does PCA work, and what are its key applications? (5 marks)\n",
    "\n",
    "c) Describe a content-based recommendation system. How does it work, and what are its main advantages and limitations? (5 marks)\n",
    "\n",
    "d) What is Market Basket Analysis, how is it used to uncover relationships between items in transactional data? Explain the key concepts and techniques involved. (5 marks)\n",
    "\n",
    "\"\"\"\n",
    "rsp = inter(txt)\n",
    "print(rsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfecd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "imapth = pathlib.Path(\"D:\\Data science\\ESA 2\\ESA 2 reference questions\\Sample Papers\\TSF\\TSF_A1.jpeg\")  # Replace with your image path\n",
    "img = Image.open(imapth)\n",
    "\n",
    "# Display the image (optional)\n",
    "#disp(img)\n",
    "\n",
    "# Prepare the query and image parts\n",
    "img_prts = [\n",
    "    {\n",
    "        \"mime_type\": \"image/jpeg\",  # Or \"image/png\" if it's a PNG\n",
    "        \"data\": imapth.read_bytes()\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmpt_prts = [\n",
    "    \"Solve question number 2 with python code only, no description.\",  # Your prompt/question\n",
    "    img_prts[0],\n",
    "]\n",
    "\n",
    "# Generate content\n",
    "rsp = model.generate_content(pmpt_prts)\n",
    "\n",
    "# Print the response\n",
    "print(rsp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf52a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "imapth = pathlib.Path(\"D:\\Data science\\ESA 2\\ESA 2 reference questions\\Sample Papers\\TSF\\TSF_A2.jpeg\")  # Replace with your image path\n",
    "img = Image.open(imapth)\n",
    "\n",
    "# Display the image (optional)\n",
    "#disp(img)\n",
    "\n",
    "# Prepare the query and image parts\n",
    "img_prts = [\n",
    "    {\n",
    "        \"mime_type\": \"image/jpeg\",  # Or \"image/png\" if it's a PNG\n",
    "        \"data\": imapth.read_bytes()\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95188393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import pandas as pd\n",
      "from statsmodels.tsa.arima.model import ARIMA\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
      "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
      "\n",
      "# a) Read and preprocess data\n",
      "df = pd.read_csv('sales_data.csv')  # Replace 'sales_data.csv' with your file\n",
      "df['ds'] = pd.to_datetime(df['ds'])\n",
      "df = df.set_index('ds')\n",
      "\n",
      "# b) Time series analysis and stationarity check (Code for Dickey-Fuller, ACF, PACF omitted for brevity)\n",
      "\n",
      "# c) Split data and fit ARIMA\n",
      "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
      "model_arima = ARIMA(train['y'], order=(5, 1, 0)).fit() #Example order, adjust as needed\n",
      "predictions_arima = model_arima.predict(start=test.index[0], end=test.index[-1])\n",
      "\n",
      "rmse_arima = mean_squared_error(test['y'], predictions_arima, squared=False)\n",
      "mape_arima = mean_absolute_percentage_error(test['y'], predictions_arima)\n",
      "\n",
      "\n",
      "# 3 a) Fit exponential smoothing\n",
      "model_es = ExponentialSmoothing(train['y'], trend='add', seasonal='add', seasonal_periods=7).fit() # Example, adjust parameters as needed\n",
      "predictions_es = model_es.forecast(len(test))\n",
      "\n",
      "\n",
      "rmse_es = mean_squared_error(test['y'], predictions_es, squared=False)\n",
      "mape_es = mean_absolute_percentage_error(test['y'], predictions_es)\n",
      "\n",
      "\n",
      "# 3 b) Improve exponential smoothing (Example - optimizing parameters)\n",
      "# Code for optimization/improvement omitted for brevity\n",
      "\n",
      "# 3 c) Forecast future sales (Example with final exponential smoothing model)\n",
      "future_dates = pd.date_range(start=df.index[-1] + pd.DateOffset(1), periods=30)\n",
      "future_predictions = model_es.forecast(steps=30)\n",
      "\n",
      "future_df = pd.DataFrame({'ds': future_dates, 'y': future_predictions})\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "This code provides a basic framework. You'll need to adjust the ARIMA order, exponential smoothing parameters, and potentially add code for data cleaning, stationarity checks, and model improvement based on your specific dataset and analysis.  Also replace `'sales_data.csv'` with the actual name of your file.\n"
     ]
    }
   ],
   "source": [
    "pmpt_prts = [\n",
    "    \"Solve question number 2 with python code only, no description.\",  # Your prompt/question\n",
    "    img_prts[0],\n",
    "]\n",
    "\n",
    "# Generate content\n",
    "rsp = model.generate_content(pmpt_prts)\n",
    "\n",
    "# Print the response\n",
    "print(rsp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a034e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
